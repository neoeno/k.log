So — figuring

* What events would I like to extract?
* How can I reliably extract them from the data I have?
* Write some code to do it.

Starting from the absolute simplest:

- [x] When is the first test run?
- [x] When does the test suite fail?
- [x] When does the test suite pass?
- [-] When is any test suite run?

I also had an idea that I don't have to run through the frames sequentially.
I can run through them in some other order. Useful if I want to take only
patterns.

Let's go through in order.

## Extract: When is the first test run?

We can't match on `rspec` because when tests are written that also shows up.

We could match on `rspec` case-sensitive.

We could match on `Finished in` or `files took` or `seconds to load`.

Side-note — why on earth do some frames OCR well while others don't? In
principle they're the same image... aside from compression. Surely that can be
reversed. Anyway.

## When is any test suite run?

This one is as above, but we could match on the number of seconds finished in or
seconds to load. It's a bit RSpec specific though. Perhaps not super interesting
but it could be worthwhile knowing whether tests are run often.

We might test on the position of `Finished in` but that will change with
scrolling around to see error message etc.

The numbers OCR is pretty unreliable. We often get random permutations. I'm not
sure how much entropy there is in these numbers for us to reliably tell errors
from new runs.

## Extract: When does the test suite fail?

Random idea — we could match on colour... but a bad idea probably.

We could match on `.+ examples, .+ failures`. Sometimes those words get poorly
matched. For example, `&copy;` instead of a number or perhaps `@`, these are
manageable, but `8` is not really manageable.

We could tune over a number of frames... but there's no guarantee that it's not
always misinterpreted. That `8` will be a problem.

But actually I'm overcomplicated. If we want to detect a test suite failing we
need to detect a situation where `X examples, Y failures` where Y is non-zero.
If `Y` ends up being `8`, then the only situation we've got a green suite is
where we've got `0` or `8 examples, 0 failures`. That's relatively rare
statistically speaking so probably we can work out if there's a failure here.

## Extract: When does the test suite fail?

Opposite of above.

---

Alright — with a mix of slightly fuzzy coding and some interface tweaks, we've
got something kinda cool!

![Screenshot of interface video interface](images/Screenshot%202023-02-01%20at%2023.04.03.png)

It's not getting perfect results yet but it's pretty good. Problems I've noticed:

* If someone runs the tests, gets green, and then gets going on a new test to
  get red too fast it'll ignore the green step.
* If someone highlights green text in the terminal it won't be recognised. This
  is unfortunately reasonably common.

I think we probably need to get better at detecting test passes and failures and
noticing ones we've already seen. But this is already becoming potentially
useful.

It's also a little slow to process the text, so we probably ought to preprocess
that.

This miiiight be getting close enough for a proof of concept for this stage.
I think I need to think a bit harder about ways to improve the OCR/event
detection even further.

I think next I'll do process some of me test-driving, upload it, and then that
can be my stage one proof of concept. I'll then review whether I want to
continue this project or write it up and shelve it for another time.
