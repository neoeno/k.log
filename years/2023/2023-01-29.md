I've worked quite a lot over the past few years. Since lockdown, month by month
it got more and more out of control. I decided in mid 2022 that one way or
another this would change, and the date it would change would be Jan 1st 2023.

If you've not worked a significant management role before it might seem surprising
for it to take six months to do it. Well it does. My employer, my team, and my
self have had to prepare a fair bit.

Fortunately Jan 1st came and since then I've been working my job 9am to 6pm.
This is good. I think my loved ones honestly did not expect me to actually do it!

In any case, I find myself with a lot more time. I set myself the expectation
that I wouldn't need to decide what to do with this time immediately. I'd just
make sure I did have that time and wasn't using it to work, and trusted I'd
figure out what to do with it.

Before getting sucked into work, I was always a person with a project, even as
a teenager. Websites, art projects, largely unsuccessful novel writing, various
other bits.

So after a bit of getting used to having evenings and weekends again, plus
unfucking my habitat and reaffirming the way I want to live my relationships, I've
still got a fair bit of spare time I'm itching to use.

However, I'm also aware I can slip into old habits, so I need a couple rules to
keep me living within certain barriers. I'm pretty good with a self-imposed rule.
Flexibility is harder, but that's OK for now.

My first rule was:

1. I work my job entirely between 9am and 6pm.

And I'm going to add a rule pertaining to my outside of work projects:

2. Any project with a job dependency must happen in work hours.

So, if there's a work deadline, or someone's waiting on it, or it's going to
form part of a larger project which is important for work, it has to be done
inside work hours.

I might want to do projects that do relate to my job, and possibly will even be
used by me in my job. However if there is anything at my job that is depending
on a non-job project, or if that becomes the case at any point, I can only work
on it in work hours.

If a project is related to my job in some way, it will be a good idea to avoid the
possibility of it becoming a job-project by making it less useful. For example,
making it a proof of concept, or require significant customisation, documentation,
or integration before it can be useful.

Those 'bottom lines' are _towards_ my 'top lines' related to me having a full
life. In particular, rule 2 is related to the top line of "I work on a range of
projects inside, outside, or adjacent to work — without getting sucked in."

Another bottom line is that:

3. Every project outside of work hours must be scoped in my logs.

This is useful in general, but is in particular essential to ensure I don't get
unwittingly sucked in. Writing is a good way to exercise executive control.

---

I'd like to create a piece of software that can analyse a screen recording of
someone coding for the purposes of process review. It should:

* Be somehow consumable by a web or local interface.
* Be economical to run.
* Ideally not rely on ML cloud APIs, which will increase maintenance burden.
* Be capable of rebuilding the codebase (in full or part) that is being worked
  on, probably by capturing line numbers and such.
* Be capable of annotating particular key moments in the recording, for example
  a test suite passing or failing, an edit of a particular file or even line,
  a build, or something else.
* Produce searchable results.
* Be robust to a reasonable variety of inputs. Let's say we can fix the editor
  to be VS Code but not the theme or layout.
* Be configurable to different languages / toolsets (e.g. Ruby, Python, etc)
* Perhaps, be robust to the use of in-IDE tools (e.g. to run tests)
* Be capable of handling a variety of video input formats.
* It would be nice to consider how we could handle files (e.g config) that are
  necessary to get the code running but that aren't visible on the recording.
  Maybe some kind of start and end snapshot?

Probably a rudimentary visualisation tool will be important for this to feel
motivating to me. But not cool enough to be distracting. Maybe a browser based
video display with timestamps and text display of analysis at the current point.

Be cool to make this a mac app, but it's probably going to be distracting.

As is the general focus of my engineering practice at the moment, I should aim
to be as fast as possible (I will write up an essay at some point titled 'Speed
is a form of quality' — because it is). It might make sense to make videos of
my process also, since it'll be more fuel for the tool.

My first step is to:

- [ ] Make a proof of concept script that can detect when a test fail or pass
      has happened, and generate a list of timestamps.

There are a few potential libraries we can use.

* [The Mac Vision library](https://developer.apple.com/documentation/vision/recognizing_text_in_images)  
  Downside is that it will only run in a mac stack, I guess? Certainly web-deployment will be an issue.
* [tesserocr](https://github.com/sirfz/tesserocr)  
  Python-based. I don't intend to use Python for a web or desktop app. Probably
  we could wrap it but it will be annoying. I guess depending on how long it
  takes to actually analyse it might not matter (if we can get it instantly,
  possibly worth running not as a batch process).
* [Tesseract](https://github.com/tesseract-ocr/tesseract)  
  The main project is C++ and has a lot of, probably variable quality, wrappers.
  I suspect various wrappers will be using old versions of the underlying library.
* We couldddd train our own model for this, if we wanted to be very ambitious.
  But let's not do that.

Early potential problem — these models seem to be targetted towards natural
language (i.e. English, Spanish, etc). This makes sense for their use case
but is not so good for us. On the other hand generating training data probably
isn't so hard, so maybe we can generate a corpus to tune it if needed.

Early experiments with the Tesseract CLI tool confirms my suspicions. Here is a
typical result:

```
16
B47

SUIive MUSA raKEL UU

xit ‘adds song to tracker' do
music_tracker = MusicTracker.new
music_tracker.add( ‘Hello - adele")
expect (music_tracker.list).to eq ['Hello - adele")
end
```

Promising in the sense that we can probably search and scan using output like
this, with a few tweaks (it likes to count `0` as `©`). However we clearly won't
be able to recreate a codebase with this sort of output.

Two issues present themselves:

1. The models are tuned for natural language, not code, and so produce poor
   output.
2. The segmentation of panes is not great. We definitely can't locate line numbers
   in this form.

I also checked with the built-in Mac OCR — a little but not much better. I
didn't check the cloud tools because their pricing is as high as $0.10 per minute. That's way off what my purposes require.

So in the short term, our feature list can include:

> Automatically process a coding video to extract features like test runs and 
> snap to them.

That's still pretty nice. If I wanted to take it further I'd need to:

1. Segment up frames into sections using some computer vision tool.
2. Use a similar technique to extract line numbers.
3. Train a custom model to OCR on code with typical code fonts.

Each of those are fairly involved tasks. But they're not unachievable — just not
actually done by anyone yet.

However, I'll do them later.

Here's an output sample I've got from scanning a typical coding video:

```
frame_000000988.png.txt:© examples, 0 failures, 1 error occurred utside of examples
frame_000000989.png.txt:© examples, 0 failures, 1 error occurred utside of examples
frame_000000990.png.txt:@ examples, 0 failures, 1 error occurred outside of examples
frame_000000991.png.txt:@ examples, 0 failures, 1 error occurred outside of examples
frame_000000992.png.txt:@ examples, 0 failures, 1 error occurred outside of examples
frame_000000993.png.txt:@ examples, 0 failures, 1 error occurred outside of examples
frame_000000994.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000000995.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000000996.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000000997.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000000998.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000000999.png.txt:© examples, 0 failures, 1 error occurred outside of examples
frame_000001000.png.txt:@ examples, 0 failures, 1 error occurred outside of examples
frame_000001005.png.txt:examples, @ failures, 4 pending
frame_000001006.png.txt:examples, @ failures, 4 pending
frame_000001007.png.txt:examples, @ failures, 4 pending
frame_000001008.png.txt:12 examples, 0 failures, 4 pending
frame_000001009.png.txt:8 end 12 examples, @ failures, 4 pending
frame_000001010.png.txt:12 examples, 0 failures, 4 pending
frame_000001011.png.txt:12 examples, 0 failures, 4 pending
frame_000001012.png.txt:12 examples, 0 failures, 4 pending
...
frame_000001214.png.txt:12 examples, 1 failure, 2 pending
frame_000001215.png.txt:12 examples, 1 failure, 2 pending
frame_000001216.png.txt:12 examples, 1 failure, 2 pending
frame_000001217.png.txt:12 examples, 1 failure, 2 pending
frame_000001218.png.txt:12 examples, 1 failure, 2 pending
frame_000001219.png.txt:18 music_tracker.add ("lie - nf*) 12 examples, i failure, 2 pending
frame_000001220.png.txt:12 examples, i failure, 2 pending
frame_000001221.png.txt:12 examples, 1 failure, 2 pending
frame_000001222.png.txt:12 examples, 1 failure, 2 pending
frame_000001223.png.txt:12 examples, 1 failure, 2 pending
frame_000001224.png.txt:12 examples, 1 failure, 2 pending
frame_000001225.png.txt:12 examples, 1 failure, 2 pending
frame_000001226.png.txt:12 examples, 1 failure, 2 pending
frame_000001227.png.txt:12 examples, 1 failure, 2 pending
frame_000001228.png.txt:12 examples, 1 failure, 2 pending
frame_000001229.png.txt:12 examples, 1 failure, 2 pending
frame_000001230.png.txt:12 examples, 1 failure, 2 pending
```

Plenty good enough for our current purposes!

One issue is that it's very slow. I'm writing this as it's executing. Probably
at least 1x processing time to video length, maybe slower. We do have the time
to burn but I'd still like it to be a bit faster. My laptop is currently not
anywhere near maxed out on CPU so probably it's parallelisable. Plus I'm in the
CLI running a new command per frame so that's going to introduce some overhead.

Running `xargs` with `-P8` seems to saturate CPU just fine and is much faster.
Still kinda slow but it's better. 6m08s for a 24m video. Okaayyy.

So we've got something workwithable. I think the next step is to:

1. Come up with a reproducible script that will output a list of frames and 
   their text in a way that can be imported into JS.
2. Write some JS that will go through those frames and extract key events.
3. Create a JS tool that will play a video with a timeline of the key events.

Best command I've found so far:

```
; time ls frames | xargs -P8 -Ifile bash -c "echo file && tesseract --psm 11 frames/file pscans/file -c tessedit_char_blacklist="®©@" 2> /dev/null"
```

The `tessedit_char_blacklist` does away with `®`, `©` and `@`. We're going to 
need to have that `@` back at some point but for now since we're not extracting
code we'll be better to have it prefer numbers.

The `--psm 11` tells it to find all the text from the image and dump it in the
file, don't attempt to lay it out proper. This works for us for now and helps
increase the accuracy of finding `rspec` runs.

We're still getting some occasional wrong recognitions for the digits. Most
notably `i` for `1` which we can 'fix in post'.

Actually, I'm going to ditch the blacklist. At least having them there we know
they're zero. If we blacklist them sometimes there's no character detected
at all.

